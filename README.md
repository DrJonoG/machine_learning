# Machine Learning Course Notes

This repository contains comprehensive notes and concepts from the Machine Learning course at Imperial College London. The course covers fundamental topics in machine learning, providing both theoretical foundations and practical applications.

## File Overview

### Module_002 - Introduction.ipynb
- Introduces the basic concepts of machine learning
- Covers supervised vs unsupervised learning
- Explains key terminology and the machine learning workflow

### Module_003 - Probability.ipynb
- Focuses on probability theory fundamentals
- Covers probability distributions, Bayes' theorem, and conditional probability
- Explains how probability relates to machine learning models

### Module_004 - Statistics.ipynb
- Covers statistical concepts essential for machine learning
- Includes hypothesis testing, confidence intervals, and statistical significance
- Explains descriptive and inferential statistics

### Module_005 - Bias-Variance.ipynb
- Explores the bias-variance tradeoff in machine learning
- Discusses model complexity and generalisation
- Covers techniques to balance bias and variance

### Module_006 - Performance Evaluation.ipynb
- Focuses on evaluating machine learning models
- Covers metrics like accuracy, precision, recall, and F1-score
- Explains confusion matrices and ROC curves

### Module_007 - Cross Validation.ipynb
- Explains cross-validation techniques for model evaluation
- Covers k-fold cross-validation and stratified sampling
- Discusses oversampling techniques for imbalanced datasets

### Module_008 - KNN.ipynb
- Explains the k-Nearest Neighbors algorithm and its implementation
- Covers distance metrics and neighbor selection strategies
- Discusses parameter tuning and the effect of k on model performance

### Module_009 - Decision Trees.ipynb
- Introduces decision tree algorithms and their construction
- Explains splitting criteria and tree growth mechanisms
- Covers pruning techniques and handling categorical variables

### Module_010 - Tree Ensembles.ipynb
- Explores advanced tree-based methods including Random Forests and AdaBoost
- Covers bootstrapping and feature importance evaluation techniques
- Discusses model interpretability and the bias-variance tradeoff in ensembles

### Module_011 - Naive Bayes.ipynb
- Explains the Naive Bayes classification algorithm
- Covers probability-based classification and the independence assumption
- Discusses applications and limitations of Naive Bayes

### Module_012 - Bayesian Optimisation.ipynb
- Introduces Bayesian optimisation techniques for hyperparameter tuning
- Covers acquisition functions and surrogate models
- Explains the exploration-exploitation tradeoff

### Module_013 - Logistic Regression.ipynb
- Covers logistic regression for binary classification
- Explains maximum likelihood estimation and model training
- Discusses regularisation and model interpretation

### Summary.ipynb
- Provides a comprehensive summary of all course modules
- Includes key concepts, formulas, and practical applications
- Serves as a quick reference guide for the entire course

## Usage
These Jupyter notebooks can be used as study materials, reference guides, or starting points for machine learning projects. Each notebook contains detailed explanations, examples, and important concepts from the course.

## License
These materials are shared with permission from Imperial College London. Please use them responsibly and in accordance with the course's academic integrity policies.
